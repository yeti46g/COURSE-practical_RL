{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/800px-Markov_Decision_Process.svg.png' width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital state = s0\n",
      "next_state = s2, reward=0.0,done=False\n"
     ]
    }
   ],
   "source": [
    "print('inital state =', mdp.reset())\n",
    "next_state,reward,done,info = mdp.step('a1')\n",
    "print('next_state = %s, reward=%s,done=%s'%(next_state,reward,done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions(\"s1\") = ('a0', 'a1')\n",
      "mdp.get_next_states(\"s1\",\"a0\") = {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_transition_prob(\"s1\",\"a0\",\"s0\") = 0.7\n"
     ]
    }
   ],
   "source": [
    "print('mdp.get_all_states =',mdp.get_all_states())\n",
    "print('mdp.get_possible_actions(\"s1\") =',mdp.get_possible_actions('s1'))\n",
    "print('mdp.get_next_states(\"s1\",\"a0\") =',mdp.get_next_states('s1','a0'))\n",
    "print('mdp.get_transition_prob(\"s1\",\"a0\",\"s0\") =', mdp.get_transition_prob('s1','a0','s0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp,state_values,state,action,gamma):\n",
    "    \n",
    "    Q = 0\n",
    "    next_states = mdp.get_next_states(state,action)\n",
    "   \n",
    "    for next_state in next_states:\n",
    "        probability = next_states[next_state]\n",
    "        reward = mdp.get_reward(state,action,next_state)\n",
    "        Q += probability*(reward + gamma*state_values[next_state])\n",
    "   \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    if mdp.is_terminal(state): return 0\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    state_value = 0\n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        if action_value > state_value:\n",
    "            state_value = action_value\n",
    "    \n",
    "    return  state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "num_iter = 100\n",
    "min_difference = 0.001\n",
    "\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "new_state_values = {}\n",
    "for i in range(num_iter):\n",
    "    for s in state_values:\n",
    "        new_state_values[s] = get_new_state_value(mdp,state_values,s,gamma)\n",
    "    assert isinstance(new_state_values,dict)\n",
    "    diffs = [abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states()]\n",
    "    diff = max(abs(new_state_values[s]-state_values[s]) for s in mdp.get_all_states())\n",
    "    print('iter %4i | diff:%6.5f | '%(i,diff),end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\"%(s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "    state_values = dict(new_state_values)\n",
    "    if diff < min_difference:\n",
    "        print('Terminated'); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "   \n",
    "    if mdp.is_terminal(state): return None\n",
    "     \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    optimal_action = None\n",
    "    optimal_action_value = - float(\"inf\")\n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        if action_value >= optimal_action_value:\n",
    "            optimal_action_value = action_value\n",
    "            optimal_action = action\n",
    "    \n",
    "    return optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_action = get_optimal_action(mdp, state_values, 's1', gamma=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
